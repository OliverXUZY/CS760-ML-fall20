{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  Siblings/Spouses Aboard  \\\n",
       "0         0       3    0  22.0                        1   \n",
       "1         1       1    1  38.0                        1   \n",
       "2         1       3    1  26.0                        0   \n",
       "3         1       1    1  35.0                        1   \n",
       "4         0       3    0  35.0                        0   \n",
       "\n",
       "   Parents/Children Aboard     Fare  \n",
       "0                        0   7.2500  \n",
       "1                        0  71.2833  \n",
       "2                        0   7.9250  \n",
       "3                        0  53.1000  \n",
       "4                        0   8.0500  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('titanic_data.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k, dis = None):\n",
    "        self.k = k\n",
    "        if dis == None:\n",
    "            self.dis = self._euclidean\n",
    "        \n",
    "    def _euclidean(self, x, y):\n",
    "        return np.sqrt(np.sum((x - y)**2))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.feature = X\n",
    "        self.label = y\n",
    "        self.uniq = np.unique(y)\n",
    "        \n",
    "    def nearest(self, new_single):   ## find nearest indices\n",
    "        train_dis = self.feature.apply(lambda x: self.dis(new_single, x), axis = 1)\n",
    "        return train_dis.sort_values().index     #  return the index of nearest value, np.argsort() cannot handle when self.feature is shuffled\n",
    "        \n",
    "    def predict(self, new):\n",
    "        if new.ndim == 1:\n",
    "            #print(self.nearest(new))\n",
    "            k_ind = self.nearest(new)[:self.k]\n",
    "            k_label = self.label[k_ind]\n",
    "            count = np.bincount(k_label, minlength = self.uniq.shape[0])\n",
    "            #print(count)\n",
    "            pre = np.argmax(count)\n",
    "            return pre\n",
    "        \n",
    "        pre = np.ones(new.shape[0])\n",
    "        for i in range(new.shape[0]):\n",
    "            #print(new)\n",
    "            k_ind = self.nearest(new.iloc[i])[:self.k]\n",
    "            k_label = self.label[k_ind]\n",
    "            #print(k_ind)\n",
    "            count = np.bincount(k_label, minlength = self.uniq.shape[0])\n",
    "            \n",
    "            pre[i] = np.argmax(count)\n",
    "            \n",
    "        return pre    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = KNN(int(round(np.sqrt(data.shape[0]))))\n",
    "K.fit(data.drop(['Survived'],axis = 1), data.Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use Euclidean distance, I think it's standard if we do not know the exact distribution of the data. The Euclidean distance might not be suitable for discrete data (especially  binary data). I think I can improve my classifier with better distance in future research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my own feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.Series({'Pclass':3, 'Sex':0, 'Age': 23, 'Siblings/Spouses Aboard': 1, 'Parents/Children Aboard':0, 'Fare': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.predict(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would not survived the Titanic sinking based on the KNN prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use square root of N as the choice of K, where N is the total number of samples. I implemented KNN with several K and the square root of N gave the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(round(np.sqrt(data.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used cross validation to assess the realiablility of my model. I divided my data into 10 folds and compute the accuracy on test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_cross_val(data, fold = 10):\n",
    "        indices = np.random.permutation(list(data.index))\n",
    "        testsize = data.shape[0]//fold\n",
    "        acc = []\n",
    "        for i in range(fold):\n",
    "            if i == fold - 1:\n",
    "                test = data.loc[indices[i*testsize:]]\n",
    "                train = data.loc[data.index.isin(indices[i*testsize:]) == False]\n",
    "            else:\n",
    "                test = data.loc[indices[i*testsize : (i+1)*testsize]]\n",
    "                train = data.loc[data.index.isin(indices[i*testsize : (i+1)*testsize]) == False]\n",
    "            \n",
    "            #print(train.shape)\n",
    "            K = KNN(k = 30)\n",
    "            K.fit(train.drop(['Survived'],axis = 1), train.Survived)\n",
    "            \n",
    "            pre = K.predict(test.drop(['Survived'],axis = 1))\n",
    "            \n",
    "            acc.append((pre == test.Survived).sum()/len(test.Survived))\n",
    "            \n",
    "        #return np.array(acc).mean()\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6931818181818182,\n",
       " 0.6818181818181818,\n",
       " 0.7386363636363636,\n",
       " 0.7159090909090909,\n",
       " 0.7613636363636364,\n",
       " 0.6931818181818182,\n",
       " 0.6818181818181818,\n",
       " 0.7159090909090909,\n",
       " 0.6931818181818182,\n",
       " 0.7157894736842105]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = KNN_cross_val(data)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7090789473684211"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got accuracy around 70% in each fold. The model predict farely well on this data. In future work, I may improve the accuracy of my model by specifying the distance of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.data = data\n",
    "        self.data0 = data[data.Survived == 0]\n",
    "        self.data1 = data[data.Survived == 1]\n",
    "        \n",
    "        self.py0 = self.data0.shape[0]/self.data.shape[0]\n",
    "        self.py1 = 1 - self.py0\n",
    "        \n",
    "        \n",
    "        self.y0 = self.data0.Survived\n",
    "        self.X0 = self.data0.drop(['Survived'],axis = 1)\n",
    "        \n",
    "        self.y1 = self.data1.Survived\n",
    "        self.X1 = self.data1.drop(['Survived'],axis = 1)\n",
    "        \n",
    "        n0 = self.data0.shape[0]  ## sample size\n",
    "        n1 = self.data1.shape[0]  ## sample size\n",
    "        \n",
    "        \n",
    "        # Pclass [1,2,3]\n",
    "        self.y0_Pclass = [(self.X0.Pclass == 1).sum()/n0]\n",
    "        self.y0_Pclass.append((self.X0.Pclass == 2).sum()/n0)\n",
    "        self.y0_Pclass.append((self.X0.Pclass == 3).sum()/n0)\n",
    "        \n",
    "        self.y1_Pclass = [(self.X1.Pclass == 1).sum()/n1]\n",
    "        self.y1_Pclass.append((self.X1.Pclass == 2).sum()/n1)\n",
    "        self.y1_Pclass.append((self.X1.Pclass == 3).sum()/n1)\n",
    "        \n",
    "        # Sex  [0,1]\n",
    "        self.y0_Sex = [(self.X0.Sex == 0).sum()/n0]\n",
    "        self.y0_Sex.append((self.X0.Sex == 1).sum()/n0)\n",
    "        \n",
    "        self.y1_Sex = [(self.X1.Sex == 1).sum()/n1]\n",
    "        self.y1_Sex.append((self.X1.Sex == 0).sum()/n1)\n",
    "        \n",
    "        # Age [mu,sigma]\n",
    "        self.y0_Age = [self.X0.Age.mean()]  # mu\n",
    "        self.y0_Age.append(self.X0.Age.std(ddof = 0)) # sigma\n",
    "        \n",
    "        self.y1_Age = [self.X1.Age.mean()]\n",
    "        self.y1_Age.append(self.X1.Age.std(ddof = 0))\n",
    "        \n",
    "        \n",
    "        # Siblings/Spouses Aboard\n",
    "        self.y0_Sib = (self.X0['Siblings/Spouses Aboard']).mean()  # lambda\n",
    "        \n",
    "        self.y1_Sib = (self.X1['Siblings/Spouses Aboard']).mean()  # lambda\n",
    "        \n",
    "        # Parents/Children Aboard\n",
    "        self.y0_Par = (self.X0['Parents/Children Aboard']).mean()  # lambda\n",
    "        \n",
    "        self.y1_Par = (self.X1['Parents/Children Aboard']).mean()  # lambda\n",
    "        \n",
    "        # Fare [mu,sigma]\n",
    "        self.y0_Fare = [self.X0.Fare.mean()]  # mu\n",
    "        self.y0_Fare.append(self.X0.Fare.std(ddof = 0)) # sigma\n",
    "        \n",
    "        self.y1_Fare = [self.X1.Fare.mean()]  # mu\n",
    "        self.y1_Fare.append(self.X1.Fare.std(ddof = 0)) # sigma\n",
    "        \n",
    "    def _predict(self, new = None):\n",
    "        # compute prob of y0(just numerator)\n",
    "        y0_prob = [self.py0]\n",
    "        y0_prob.append(self.y0_Pclass[new.Pclass.astype(int) - 1])  # prob of Pclass\n",
    "        y0_prob.append(self.y0_Sex[new.Sex.astype(int) - 1])  # prob of Sex\n",
    "        y0_prob.append(1/self.y0_Age[1] * np.exp( - np.square(new.Age - self.y0_Age[0])/2/np.square(self.y0_Age[1])))  # prob of Age\n",
    "        y0_prob.append(np.power(self.y0_Sib , new['Siblings/Spouses Aboard']) * np.exp(-self.y0_Sib) / \\\n",
    "                       np.math.factorial(new['Siblings/Spouses Aboard']))  # Siblings/Spouses Aboard\n",
    "        \n",
    "        y0_prob.append(np.power(self.y0_Par , new['Parents/Children Aboard']) * np.exp(-self.y0_Par) / \\\n",
    "                       np.math.factorial(new['Parents/Children Aboard']))  # Parents/Children Aboard\n",
    "        \n",
    "        y0_prob.append(1/self.y0_Fare[1] * np.exp( - np.square(new.Fare - self.y0_Fare[0])/2/np.square(self.y0_Fare[1])))  # prob of Fare\n",
    "        \n",
    "        # compute prob of y1(just numerator)\n",
    "        y1_prob = [self.py1]\n",
    "        y1_prob.append(self.y1_Pclass[new.Pclass.astype(int) - 1])  # prob of Pclass\n",
    "        y1_prob.append(self.y1_Sex[new.Sex.astype(int) - 1])  # prob of Sex\n",
    "        y1_prob.append(1/self.y1_Age[1] * np.exp( - np.square(new.Age - self.y1_Age[0])/2/np.square(self.y1_Age[1])))  # prob of Age\n",
    "        y1_prob.append(np.power(self.y1_Sib , new['Siblings/Spouses Aboard']) * np.exp(-self.y1_Sib) / \\\n",
    "                       np.math.factorial(new['Siblings/Spouses Aboard']))  # Siblings/Spouses Aboard\n",
    "        \n",
    "        y1_prob.append(np.power(self.y1_Par , new['Parents/Children Aboard']) * np.exp(-self.y1_Par) / \\\n",
    "                       np.math.factorial(new['Parents/Children Aboard']))  # Parents/Children Aboard\n",
    "        \n",
    "        y1_prob.append(1/self.y1_Fare[1] * np.exp( - np.square(new.Fare - self.y1_Fare[0])/2/np.square(self.y1_Fare[1])))  # prob of Fare\n",
    "        \n",
    "        \n",
    "        return  0 if np.prod(y0_prob) >= np.prod(y1_prob) else 1\n",
    "        \n",
    "    def predict(self, new = None):\n",
    "        if new.ndim == 1:\n",
    "            return self._predict(new)\n",
    "        \n",
    "        pre = np.ones(new.shape[0])\n",
    "        for i in range(new.shape[0]):\n",
    "            pre[i] = self._predict(new.iloc[i])\n",
    "        return pre \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = NaiveBayes()\n",
    "NB.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 values in **Pclass**, I assume them as multinomial distribution.\n",
    "\n",
    "There are binary digits in **Sex**, I assume them as Bernoulli distribution.\n",
    "\n",
    "The data in **age** range from 0.42 to 80, I assume them as normal distribution.\n",
    "\n",
    "There are 9 values (0-8) in **Siblings/Spouses Aboard**, I assume them as poisson distribution.\n",
    "\n",
    "There are 7 values (0-6) in **Parents/Children Aboard**, I assume them as poisson distribution.\n",
    "\n",
    "The data in **Fare** range from 0 to 512.3, I assume them as normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my own feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass                      3\n",
       "Sex                         0\n",
       "Age                        23\n",
       "Siblings/Spouses Aboard     1\n",
       "Parents/Children Aboard     0\n",
       "Fare                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = pd.Series({'Pclass':3, 'Sex':0, 'Age': 23, 'Siblings/Spouses Aboard': 1, 'Parents/Children Aboard':0, 'Fare': 0})\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB.predict(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would not survived the Titanic sinking based on the Naive Bayes prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used cross validation to assess the realiablility of my model. I divided my data into 10 folds and compute the accuracy on test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_cross_val(data, fold = 10):\n",
    "        indices = np.random.permutation(list(data.index))\n",
    "        testsize = data.shape[0]//fold\n",
    "        acc = []\n",
    "        NB = NaiveBayes()\n",
    "        for i in range(fold):\n",
    "            if i == fold - 1:\n",
    "                test = data.loc[indices[i*testsize:]]\n",
    "                train = data.loc[data.index.isin(indices[i*testsize:]) == False]\n",
    "            else:\n",
    "                test = data.loc[indices[i*testsize : (i+1)*testsize]]\n",
    "                train = data.loc[data.index.isin(indices[i*testsize : (i+1)*testsize]) == False]\n",
    "            \n",
    "            #print(train.shape)\n",
    "            \n",
    "            NB.fit(train)\n",
    "            \n",
    "            pre = NB.predict(test.drop(['Survived'],axis = 1))\n",
    "            \n",
    "            acc.append((pre == test.Survived).sum()/len(test.Survived))\n",
    "            \n",
    "        #return np.array(acc).mean()\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7272727272727273,\n",
       " 0.625,\n",
       " 0.6477272727272727,\n",
       " 0.625,\n",
       " 0.7045454545454546,\n",
       " 0.625,\n",
       " 0.6590909090909091,\n",
       " 0.6022727272727273,\n",
       " 0.7272727272727273,\n",
       " 0.6210526315789474]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = NB_cross_val(data)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6564234449760765"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got accuracy around 66% in each fold. The model predict farely well on this data. In future work, I may improve the accuracy of my model by specifying the distribution of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prefer Random Forest and Decision Tree model, the random forest has the highest accuracy and is robust to different kinds of data. The random forest also does not require any strong assumption on the features. The accuracy of Random Forest can still be improved if we specify each feature carefully instead of just simplifying them into binary digits.\n",
    "\n",
    "The Logistic regression trains data in a more statistical way, it assumes data as i.i.d Bernoulli distribution, has the explicit formula for log-likelihood, we use gradient descent to compute MLE. In this method, we can predict the probability of each new observation. But each feature has its own property and range, I think it would be better if I standardized each feature before training the data.\n",
    "\n",
    "The KNN method is a lazy algorithm. The computation complexity grows drastically as the number of features growing (curse of dimensionality). Combining these 2 aspects, the training time would be extremely long. We also need to be careful in choosing the right K.\n",
    "\n",
    "The Naive Bayes did not work so well on this dataset. It also requires data having independent features. One thing we need to be careful about is if one category in the test set did not show in the train set, the probability will zero out. We can add some regularization to avoid this.\n",
    "\n",
    "In a nutshell, the choice of methods really depends on the data. We need to know the data well and use the method most suitable for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
